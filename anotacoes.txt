Validation split: 0.2
model.compile(
    optimizer="adam",
    loss='sparse_categorical_crossentropy',
    metrics=["accuracy"]
)
    NUM_CATEGORIES = 12
    IMG_WIDTH = 64
    IMG_HEIGHT = 64
    EPOCHS = 10
    BATCH_SIZE = 32


1o Teste:
    Modelo usado foi o aprendido a partir do Curso de Harvard
    model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dense(12, activation = "softmax")
    ])
   
    accuracy: 0.8896 - loss: 0.3218 - val_accuracy: 0.7821 - val_loss: 0.8899
    identificado overfitting

2o teste:
    outra camada de dropout de 0.5, visando diminuir overfitting. Não adiantou e piorou acuracia e validation accuracy

        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(12, activation = "softmax")

3o teste:
    aumentado a largura e altura das imagens de 64 para 224. Tive que remover guardar o dataset na RAM pois estava faltando memoria (removi o .cache() dos dados de treinamento)
    acuracia aumentou um pouco, mas a validacao piorou

    NUM_CATEGORIES = 12
    IMG_WIDTH = 224
    IMG_HEIGHT = 224
    EPOCHS = 10
    BATCH_SIZE = 32
    
    tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(12, activation = "softmax")
        
    accuracy: 0.9057 - loss: 0.2936 - val_accuracy: 0.7438 - val_loss: 1.0677

4o teste: tecnicas de data augmentation aplicadas(random flip e random rotation). training accuracy cai de 90 para 68, mas modelo para de ter overfitting 
pois validation accuracy fica em 69

    data_augmentation = tf.keras.Sequential([
        tf.keras.layers.RandomFlip("horizontal_and_vertical"),
        tf.keras.layers.RandomRotation(0.2)
    ])

    def get_model():
        model = tf.keras.Sequential([
            tf.keras.layers.Rescaling(1./255),
            data_augmentation,
            tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
            tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
            tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(512, activation="relu"),
            tf.keras.layers.Dropout(0.5),
            tf.keras.layers.Dense(256, activation="relu"),
            tf.keras.layers.Dropout(0.5),
            tf.keras.layers.Dense(12, activation = "softmax")
        ])
        
        model.compile(
            optimizer="adam",
            loss='sparse_categorical_crossentropy',
            metrics=["accuracy"]
        )
        return model

    
5o teste:
    dataset dividido em treino, validacao e teste. Todas as imagens das 3 classes de vidro (brown, green e white) foram juntadas em 1 classe só chamada glass.
    adicionada camada de data augmentation.
    339/339 ━━━━━━━━━━━━━━━━━━━━ 23s 68ms/step - accuracy: 0.6435 - loss: 1.0970 - val_accuracy: 0.6199 - val_loss: 1.1166
    74/74 ━━━━━━━━━━━━━━━━━━━━ 2s 21ms/step - accuracy: 0.6362 - loss: 1.1140
    def get_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandAugment(value_range=(0, 1), num_ops=2, factor=0.5, interpolation="bilinear", seed=123, data_format=None,),
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])
    
    model.compile(
        optimizer="adam",
        loss='sparse_categorical_crossentropy',
        metrics=["accuracy"]
    )

6o teste:
    como a camada de data augmentation piorou o aprendizado, neste teste reduzi o factor da camada RandAugment de 0.5 para 0.2.
    Implementado o earlystopping com min_delta de 0.01 e patience de 5. A validation accuracy melhorou, atingindo 75,29% na 10 época, mas caiu para 72,31 ao final da 15 época
    Epoch 15/50
    339/339 ━━━━━━━━━━━━━━━━━━━━ 23s 69ms/step - accuracy: 0.9196 - loss: 0.2473 - val_accuracy: 0.7231 - val_loss: 1.1112
    Epoch 15: early stopping
    74/74 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - accuracy: 0.7332 - loss: 1.0653
    callback = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.01,
        patience=5,
        verbose=1,
        mode='auto',
        baseline=None,
        restore_best_weights=False,
        start_from_epoch=0
    )
    def get_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandAugment(value_range=(0, 1), num_ops=2, factor=0.2, interpolation="bilinear", seed=123, data_format=None,),
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])


7oteste:
    implementado kernel regularizer L1L2 com valor default em todas as camadas densas e convolucionais. patience do earlystopping reduzido para de 5 para 3.
    Acuraia de valdacao atingiu 76,6% na 10a epoca, e terminou em 75,33% na 12a epoca. acuracia no teste ficou em 0.76
    339/339 ━━━━━━━━━━━━━━━━━━━━ 24s 70ms/step - accuracy: 0.7428 - loss: 0.7615 - val_accuracy: 0.7533 - val_loss: 0.7686
    Epoch 12: early stopping
    74/74 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.7687 - loss: 0.7446
    def get_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandAugment(value_range=(0, 1), num_ops=2, factor=0.2, interpolation="bilinear", seed=123, data_format=None,),
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(256, activation="relu", kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu", kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])
    callback = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.01,
        patience=3,
        verbose=1,
        mode='auto',
        baseline=None,
        restore_best_weights=False,
        start_from_epoch=0
    )

    8o teste: 
        Treinei o mesmo modelo 2 vezes com o mesmo dataset e obtive grandes diferencas de acuracia. Para resolver essa instabilidade, resolvi reduzir a taxa de aprendizado ao longo do treinamento.
        Para isso foi utilizado o callback ReduceLROnPlateau
        Resultado:

        Epoch 10/50
        339/339 ━━━━━━━━━━━━━━━━━━━━ 24s 70ms/step - accuracy: 0.8515 - loss: 0.4444 - val_accuracy: 0.7256 - val_loss: 0.9851 - learning_rate: 0.0010
        Epoch 10: early stopping
        74/74 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.7405 - loss: 0.9276

        def get_model():
            model = tf.keras.Sequential([
                tf.keras.layers.Rescaling(1./255),
                tf.keras.layers.RandAugment(value_range=(0, 1), num_ops=2, factor=0.2, interpolation="bilinear", seed=123, data_format=None,),
                
                #1o bloco
                tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2),
                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                
                #2o bloco
                tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2),
                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                
                #3o bloco
                tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2),
                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(256, activation="relu", kernel_regularizer=tf.keras.regularizers.L1L2),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(256, activation="relu", kernel_regularizer=tf.keras.regularizers.L1L2),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
            ])

                reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_accuracy', 
            factor=0.2, # Reduz a taxa de aprendizado por um fator de 5 (1 * 0.2 = 0.2)
            patience=3, # Nº de épocas sem melhora antes de reduzir
            min_lr=0.00001, # Limite mínimo para a taxa de aprendizado
            verbose=1
        )
            callback = tf.keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                min_delta=0.01,
                patience=5,
                verbose=1,
                mode='auto',
                baseline=None,
                restore_best_weights=False,
                start_from_epoch=0
            )
            
            model.fit(
                training_set,
                validation_data=validation_set,
                epochs=EPOCHS,
                callbacks=[callback, reduce_lr]
            )

    9o teste:
    Quis acelerar o tempo de treinamento do modelo. Para isso ativei o treinamento com precisao mista e utilizei o metodo .cache().
     O tempo de treinamento medio das epocas anteriores estava em 24s
    Resultado: o tempo de treinamento piorou. A acuracia ainda esta variando MUITO ao longo dos treinamentos. por isso, para o proximo treinamento implementarei batch normalization,
     reduzirei a quantidade de camadas de dropout e utilizarei apenas a regularizacao de pesos L2, com um valor pequeno
     No teste atual foram inseridas as seguintes linhas:

     tf.keras.mixed_precision.set_global_policy('mixed_float16') - treinamento com precisao mista
     optimizer = tf.keras.optimizers.Adam()
     optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)
    
    train_dataset = train_dataset.cache("cache_treino").prefetch(buffer_size=AUTOTUNE)
    validation_dataset = validation_dataset.cache("cache_validacao").prefetch(buffer_size=AUTOTUNE)
    test_dataset = test_dataset.cache("cache_teste").prefetch(buffer_size=AUTOTUNE) - salvar em cache 
    Modelo ainda se manteve o mesmo:
    model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandAugment(value_range=(0, 1), num_ops=2, factor=0.2, interpolation="bilinear", seed=123, data_format=None,),
        
        #1o bloco
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #2o bloco
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #3o bloco
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(256, activation="relu", kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu", kernel_regularizer=tf.keras.regularizers.L1L2),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])

    10o teste:

    Implementeado batch normalization, reduzida camadas de dropout e camadas densa de classificacao. learning rate mantido em 0.001.
     Regularizacao de pesos somente L2 com valor pequeno.
     O desempenho do modelo caiu muito, e o tempo de treinamento continua alto. 

     Resultado: desempenho do modelo caiu muito e o treinamento esta instavel, com acuracias de validacao e treinamento variando muito entre épocas. 

    11o Teste: Como o desempenho do modelo piorou muito com batch normalization (provalmente devido a diferenca da quantidade de imagens entre classes), voltei a primeira arquitetura
     e implementei camadas de data augmentation e mantive o callback de early stopping.
    Resultado: Melhor modelo ate agora. Treinado durante 20 epocas e parado por early stopping. Atingiu acuracia de validacao de 77,59% e acuracia no teste de 78,89%
    Epoch 20/50
    339/339 ━━━━━━━━━━━━━━━━━━━━ 27s 79ms/step - accuracy: 0.7773 - loss: 0.6753 - val_accuracy: 0.7759 - val_loss: 0.7209
    Epoch 20: early stopping
    74/74 ━━━━━━━━━━━━━━━━━━━━ 1s 19ms/step - accuracy: 0.7889 - loss: 0.6565
     Modelo:

     model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.1),
        tf.keras.layers.RandomZoom(0.1),
        
        #1o bloco
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #2o bloco
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #3o bloco
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])

    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=["accuracy"]
    )

    callback = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.01,
        patience=5,
        verbose=1,
        mode='auto',
        baseline=None,
        restore_best_weights=False,
        start_from_epoch=0
    )

    12o teste:
    Introduzida reducao de learning rate durante o treinamento, alem do aumento de data augmentation.
    Resultado: Acuracia no teste se manteve ligeiramente inferior em relacao ao 11o teste. Acuracia de validacao se manteve ligeiramente superior.
    Epoch 20/50
    339/339 ━━━━━━━━━━━━━━━━━━━━ 28s 82ms/step - accuracy: 0.8077 - loss: 0.5657 - val_accuracy: 0.7725 - val_loss: 0.6803 - learning_rate: 2.5000e-04
    Epoch 21/50
    339/339 ━━━━━━━━━━━━━━━━━━━━ 28s 81ms/step - accuracy: 0.8082 - loss: 0.5547 - val_accuracy: 0.7789 - val_loss: 0.6559 - learning_rate: 2.5000e-04
    Epoch 21: early stopping
    74/74 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.7873 - loss: 0.6738

    modelo:

    model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.15),
        tf.keras.layers.RandomZoom(0.15),
        tf.keras.layers.RandomContrast(0.1),
        
        #1o bloco
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #2o bloco
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #3o bloco
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])

    callback = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.01,
        patience=5,
        verbose=1,
        mode='auto',
        baseline=None,
        restore_best_weights=False,
        start_from_epoch=0
    )
    
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,     # Reduz para metade
    patience=2,     # Espera 2 épocas sem melhorar
    min_lr=1e-6,
    verbose=1
)

13o teste:
Sera introduzida camada GlobalAveragePooling2D apos as camadas convolucionais e sera removida a camada de flatten, visando melhor generalizacao.
Resultado:Melhorou a generalizacao, mas pirou bastante a acuracia. 
Epoch 19/50
339/339 ━━━━━━━━━━━━━━━━━━━━ 19s 57ms/step - accuracy: 0.7025 - loss: 0.8727 - val_accuracy: 0.7060 - val_loss: 0.8814 - learning_rate: 6.2500e-05
Epoch 19: early stopping
74/74 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - accuracy: 0.7166 - loss: 0.8706

modelo:
model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.15),
        tf.keras.layers.RandomZoom(0.15),
        tf.keras.layers.RandomContrast(0.1),
        
        #1o bloco
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #2o bloco
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #3o bloco
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])
    
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=["accuracy"]
    )
    
    callback = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.01,
        patience=5,
        verbose=1,
        mode='auto',
        baseline=None,
        restore_best_weights=False,
        start_from_epoch=0
    )
    
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,     # Reduz para metade
    patience=2,     # Espera 2 épocas sem melhorar
    min_lr=1e-6,
    verbose=1
)

14o teste:
incluido mais um bloco convolucional e GlobalAveragePooling2D mantida. 

resultado: 2o melhor desempenho no teste e validacao:

Epoch 25/50
339/339 ━━━━━━━━━━━━━━━━━━━━ 22s 65ms/step - accuracy: 0.8040 - loss: 0.5710 - val_accuracy: 0.7801 - val_loss: 0.6982 - learning_rate: 1.2500e-04
Epoch 25: early stopping
74/74 ━━━━━━━━━━━━━━━━━━━━ 2s 22ms/step - accuracy: 0.7851 - loss: 0.6544

Modelo:

 model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.15),
        tf.keras.layers.RandomZoom(0.15),
        tf.keras.layers.RandomContrast(0.1),
        
        
        
        #1o bloco
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #2o bloco
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #3o bloco
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #3o bloco
        tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])
    
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=["accuracy"]
    )
    return model

    
    callback = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.01,
        patience=5,
        verbose=1,
        mode='auto',
        baseline=None,
        restore_best_weights=False,
        start_from_epoch=0
    )
    
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,     # Reduz para metade
    patience=2,     # Espera 2 épocas sem melhorar
    min_lr=1e-6,
    verbose=1
)


15o teste:
resultado:

Mantido o bloco convolucional extra, e GlobalAveragePooling2D substituida por flatten. Melhor acuracia ate agora. Um pouco de overfitting
Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
339/339 ━━━━━━━━━━━━━━━━━━━━ 25s 74ms/step - accuracy: 0.8303 - loss: 0.5046 - val_accuracy: 0.7797 - val_loss: 0.7402 - learning_rate: 5.0000e-04
Epoch 19: early stopping
74/74 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step - accuracy: 0.7984 - loss: 0.6634

model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.15),
        tf.keras.layers.RandomZoom(0.15),
        tf.keras.layers.RandomContrast(0.1),
        
        #1o bloco
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #2o bloco
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #3o bloco
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #4o bloco
        tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])
    
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=["accuracy"]
    )
    return model
    
    callback = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.01,
        patience=5,
        verbose=1,
        mode='auto',
        baseline=None,
        restore_best_weights=False,
        start_from_epoch=0
    )
    
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,     # Reduz para metade
    patience=2,     # Espera 2 épocas sem melhorar
    min_lr=1e-6,
    verbose=1
)

16o teste:
Batch size aumentado de 32 para 64. 
Resultado: melhor desempenho até agora

Epoch 20/50
170/170 ━━━━━━━━━━━━━━━━━━━━ 20s 118ms/step - accuracy: 0.8326 - loss: 0.5029 - val_accuracy: 0.8066 - val_loss: 0.5936 - learning_rate: 2.5000e-04
Epoch 21/50
170/170 ━━━━━━━━━━━━━━━━━━━━ 20s 118ms/step - accuracy: 0.8302 - loss: 0.4854 - val_accuracy: 0.8095 - val_loss: 0.5874 - learning_rate: 2.5000e-04
Epoch 21: early stopping
37/37 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.8202 - loss: 0.5793


NUM_CATEGORIES = 10
IMG_WIDTH = 224
IMG_HEIGHT = 224
EPOCHS = 50
BATCH_SIZE = 64
SEED = 123

model = tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.15),
        tf.keras.layers.RandomZoom(0.15),
        tf.keras.layers.RandomContrast(0.1),
        
        #1o bloco
        tf.keras.layers.Conv2D(32, (3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 3), padding="same", activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #2o bloco
        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #3o bloco
        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        #4o bloco
        tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation = "softmax")
    ])
    
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=["accuracy"]
    )
    return model
    
    callback = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.01,
        patience=5,
        verbose=1,
        mode='auto',
        baseline=None,
        restore_best_weights=False,
        start_from_epoch=0
    )
    
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,     # Reduz para metade
    patience=2,     # Espera 2 épocas sem melhorar
    min_lr=1e-6,
    verbose=1
)

17o teste:
Reduzido tamanho das imagens de 224x224 para 96x96. O resto do modelo se mantem o mesmo